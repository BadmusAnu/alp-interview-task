# =========================================
# Global image settings (local registry/proxy)
# =========================================
# global:
#   imageRegistry: "registry.kube-system.svc.cluster.local:5000"
  # imagePullSecrets:
  #   - name: registry-creds

# =========================================
# Enable core components
# =========================================
kubeStateMetrics:
  enabled: true

nodeExporter:
  enabled: true

kubelet:
  enabled: true
  serviceMonitor:
    enabled: true
    cAdvisor: true

kubeApiServer:
  enabled: true
  serviceMonitor:
    enabled: true

kubeControllerManager:
  enabled: true
  serviceMonitor:
    enabled: true

kubeScheduler:
  enabled: true
  serviceMonitor:
    enabled: true

kubeProxy:
  enabled: true
  serviceMonitor:
    enabled: true

coreDns:
  enabled: true
  serviceMonitor:
    enabled: true

kubeEtcd:
  enabled: false
  serviceMonitor:
    enabled: false

# =========================================
# Prometheus & default rules
# =========================================
prometheus:
  enabled: true
  service:
    type: ClusterIP
  prometheusSpec:
    retention: 15d
    scrapeInterval: 30s

    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false

    # ---- Blackbox HTTP probe for frontend uptime (requires prometheus-blackbox-exporter) ----
    additionalScrapeConfigs:
      - job_name: 'blackbox-http'
        metrics_path: /probe
        params:
          module: [http_2xx]
        static_configs:
          - targets:
              # In-cluster service endpoint to check HTTP uptime
              - 'http://python-guestbook-frontend.app.svc.cluster.local'
        relabel_configs:
          - source_labels: [__address__]
            target_label: __param_target
          - source_labels: [__param_target]
            target_label: instance
          - target_label: __address__
            replacement: 'prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115'
        scrape_interval: 30s
        scrape_timeout: 5s
    # ------------------------------------------------------------------------------------------

defaultRules:
  create: true
  rules:
    alertmanager: true
    configReloaders: true
    general: true
    k8s: true
    kubeApiserver: true
    kubeApiserverAvailability: true
    kubeApiserverSlos: true
    kubeControllerManager: true
    kubelet: true
    kubeProxy: true
    kubeScheduler: true
    kubeStateMetrics: true
    kubeletResource: true
    kubeletPod: true
    node: true
    nodeExporter: true
    prometheus: true
    prometheusOperator: true
    time: true
    etcd: false

# =========================================
# Grafana (admin creds via existing Secret)
# =========================================
grafana:
  enabled: true
  service:
    type: ClusterIP
    port: 80
  admin:
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    passwordKey: admin-password
  persistence:
    enabled: false

# =========================================
# Alertmanager (tokens/templates via Secret)
# =========================================
alertmanager:
  enabled: true
  service:
    type: ClusterIP
  alertmanagerSpec:
    portName: "http-web"
  config:
    global:
      resolve_timeout: 5m
    route:
      receiver: "null"
      group_by: ["alertname", "namespace"]
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 3h
      routes:
        - receiver: "slack"
          matchers:
            - severity =~ "warning|critical"
    receivers:
      - name: "slack"
        slack_configs:
          - api_url: https://hooks.slack.com/services/REPLACE/ME/WITH_REAL_WEBHOOK
            send_resolved: true
      - name: "null"

# =========================================
# Prometheus Operator
# =========================================
prometheusOperator:
  enabled: true

# =========================================
# Additional ServiceMonitors
# (none; app does not expose /metrics)
# =========================================
# additionalServiceMonitors: []
# =========================================
# Alert rules for uptime (internal + public)
# =========================================
additionalPrometheusRulesMap:
  uptime-rules:
    groups:
      - name: uptime.rules
        rules:
          # HTTP uptime via blackbox probe (frontend)
          - alert: HttpFrontendDown30s
            expr: probe_success{job="blackbox-http"} == 0
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Frontend HTTP down ({{ $labels.instance }})"
              description: "Blackbox HTTP probe failed (no 2xx) for 30s."
